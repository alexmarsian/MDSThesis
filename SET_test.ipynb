{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a203d1-0297-4d71-9111-ed389500c311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-05T03:08:55.646647Z",
     "iopub.status.busy": "2022-05-05T03:08:55.646178Z",
     "iopub.status.idle": "2022-05-05T03:08:57.280719Z",
     "shell.execute_reply": "2022-05-05T03:08:57.279863Z",
     "shell.execute_reply.started": "2022-05-05T03:08:55.646568Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from itop.core import Masking, CosineDecay, LinearDecay\n",
    "from dataloaders.cifar_artemis import CifarDataloader\n",
    "from train import *\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Directory for downloading and reading training data from\n",
    "datapath = Path('./data')\n",
    "if not datapath.exists():\n",
    "    datapath.mkdir(exist_ok=True)\n",
    "# Directory for saving model weights during training\n",
    "weightsDir = Path('models/weights')\n",
    "weightsDir.mkdir(exist_ok=True)\n",
    "# Directory for saving results during training\n",
    "resultsDir = Path('results')\n",
    "resultsDir.mkdir(exist_ok=True)\n",
    "\n",
    "def train(model, optimizer, data_loader, device, loss_criterion=F.cross_entropy, mask=None):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch\n",
    "    \"\"\"\n",
    "    train_loss = 0.\n",
    "    train_acc = 0\n",
    "    N = 0\n",
    "    \n",
    "    model.train()    \n",
    "    for batch_x, batch_y in data_loader:\n",
    "        # load batch onto device\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # get outputs and calculate loss\n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_criterion(outputs, batch_y)\n",
    "        # backpropagate and update optimizer\n",
    "        loss.backward()\n",
    "        if mask is not None: mask.step() # required for Sparse Evolutionary Training\n",
    "        else: optimizer.step()\n",
    "        train_loss += loss.item() * len(batch_x)\n",
    "        # calculate training accuracy\n",
    "        pred = torch.max(outputs, 1)[1]\n",
    "        train_correct = (pred == batch_y).sum()\n",
    "        train_acc += train_correct.item()\n",
    "        N += len(batch_x)\n",
    "    \n",
    "    train_loss /= N\n",
    "    train_acc /= N\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, data_loader, device, loss_criterion=F.cross_entropy):\n",
    "    \"\"\"\n",
    "    Evaluates the model for one epoch\n",
    "    \"\"\"\n",
    "    val_loss = 0.\n",
    "    val_acc = 0\n",
    "    N = 0\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = loss_criterion(outputs, batch_y)\n",
    "            val_loss += loss.item() * len(batch_x)\n",
    "            pred = torch.max(outputs, 1)[1]\n",
    "            val_correct = (pred == batch_y).sum()\n",
    "            val_acc += val_correct.item()\n",
    "            N += len(batch_x)\n",
    "    \n",
    "    val_loss /= N\n",
    "    val_acc /= N\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def run(dataset, noise_rate, noise_mode, sparsity, batch_size, \n",
    "        datapath, noise_file, weightFileName, repeats, sparse_args={}):\n",
    "    \"\"\"\n",
    "    A wrapper function to run the training loop (train, evaluate, test) with specified parameters.\n",
    "    \"\"\"\n",
    "    # intialise device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Epochs set at 200 for all non-SET experiments\n",
    "    epochs = 200 if sparse_args == {} else 200*sparse_args['multiplier']\n",
    "    \n",
    "    # running averages\n",
    "    avgTrainAcc, avgTrainLoss = [],[]\n",
    "    avgValAcc, avgValLoss = [],[]\n",
    "    avgTestAcc, avgTestLoss = [],[]\n",
    "    \n",
    "    \n",
    "    for r in range(repeats):\n",
    "        f = open(resultsDir / Path(weightFileName + f'_{r}.txt'), 'w') # save results for this run\n",
    "\n",
    "        # get data loaders for training, validation, and test sets\n",
    "        # different seed used for every repeat to split the training into train + valid set\n",
    "        dataLoader = CifarDataloader(dataset=dataset, noise_rate=noise_rate, noise_mode=noise_mode, \n",
    "                                     batch_size=batch_size, datapath=datapath, noise_file = noise_file, valid_seed = r)\n",
    "        train_loader = dataLoader.trainLoader\n",
    "        val_loader = dataLoader.validLoader\n",
    "        test_loader = dataLoader.testLoader\n",
    "        n = len(train_loader.dataset)\n",
    "\n",
    "        # initialise model and learning rate schedule\n",
    "        model, optimizer = get_model(dataset, lr=0.1, sparsity=sparsity)\n",
    "        # Set schedule for SET/RigL (based on In-Time-Over-Parameterisation paper)\n",
    "        if sparse_args != {}:\n",
    "            schedule = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2) * sparse_args['multiplier'], int(epochs * 3 / 4) * sparse_args['multiplier']], last_epoch=-1)\n",
    "        # Set schedule for non-SET models\n",
    "        else:\n",
    "            schedule = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1) # schedule based on Resnet paper\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialise mask (remains None for non-set models)\n",
    "        mask = None\n",
    "        if sparse_args != {}:\n",
    "            decay = CosineDecay(sparse_args['death_rate'], len(train_loader)*(epochs*sparse_args['multiplier']))\n",
    "            mask = Masking(optimizer, death_rate=sparse_args['death_rate'], death_mode=sparse_args['death_mode'], death_rate_decay=decay, growth_mode=sparse_args['growth_mode'],\n",
    "                           redistribution_mode=sparse_args['redistribution'], args=sparse_args) ####!!!!! Need to update Masking class to not need all the args\n",
    "            mask.add_module(model, sparse_init=sparse_args['sparse_init'], density=1-sparsity)\n",
    "\n",
    "        # save the model for best validaiton loss throughout training    \n",
    "        best_epoch, best_val_loss = 0, np.inf\n",
    "        train_loss_list, train_acc_list = [], []\n",
    "        val_loss_list, val_acc_list = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            start = time.time()\n",
    "            print(\"Epoch: {}\".format(epoch+1))\n",
    "            f.write(\"Epoch: {}\\n\".format(epoch+1))\n",
    "\n",
    "            train_loss, train_acc = train(model, optimizer, train_loader, device, mask=mask)\n",
    "            train_loss /= n\n",
    "            print(\"Train Loss: {:.6f}, Acc: {:.6f}\".format(train_loss, train_acc))\n",
    "            f.write(\"Train Loss: {:.6f}, Acc: {:.6f}\\n\".format(train_loss, train_acc))\n",
    "\n",
    "            val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "            print(\"Val Loss: {:.6f},  Acc: {:.6f}\".format(val_loss, val_acc))\n",
    "            f.write(\"Val Loss: {:.6f},  Acc: {:.6f}\\n\".format(val_loss, val_acc))\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_epoch = epoch\n",
    "                best_val_loss = val_loss\n",
    "                best_train_loss = train_loss\n",
    "                torch.save(model.state_dict(), weightsDir / Path(f'{weightFileName}_best_forward_.pkl'))\n",
    "\n",
    "            schedule.step()\n",
    "\n",
    "            train_loss_list.append(train_loss)\n",
    "            train_acc_list.append(train_acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "            val_acc_list.append(val_acc)\n",
    "\n",
    "            end = time.time()\n",
    "            print(\"Took {:.2f} minutes.\\n\".format((end - start) / 60))\n",
    "            f.write(\"Took {:.2f} minutes.\\n\".format((end - start) / 60))\n",
    "\n",
    "        print(\"Best epoch:\", best_epoch+1)\n",
    "        f.write(f\"Best epoch: {best_epoch+1}\\n\")\n",
    "        print(\"Final train loss: {:.4f}\".format(train_loss_list[best_epoch]))\n",
    "        f.write(\"Final train loss: {:.4f}\\n\".format(train_loss_list[best_epoch]))\n",
    "        print(\"Final train accuracy: {:.4f}\\n\".format(train_acc_list[best_epoch]))\n",
    "        f.write(\"Final train accuracy: {:.4f}\\n\".format(train_acc_list[best_epoch]))\n",
    "        print(\"Final val loss: {:.4f}\".format(val_loss_list[best_epoch]))\n",
    "        f.write(\"Final val loss: {:.4f}\\n\".format(val_loss_list[best_epoch]))\n",
    "        print(\"Final val accuracy: {:.4f}\\n\".format(val_acc_list[best_epoch]))\n",
    "        f.write(\"Final val accuracy: {:.4f}\\n\".format(val_acc_list[best_epoch]))\n",
    "\n",
    "        # load best model saved during training\n",
    "        model.load_state_dict(torch.load(weightsDir / Path(f'{weightFileName}_best_forward_.pkl'), map_location=device))\n",
    "        test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "        print(\"Test accuracy: {:.4f}\".format(test_acc))\n",
    "        f.write(\"Test accuracy: {:.4f}\\n\".format(test_acc))\n",
    "        \n",
    "        # save average results\n",
    "        avgTrainAcc.append(train_acc_list[best_epoch])\n",
    "        avgTrainLoss.append(train_loss_list[best_epoch])\n",
    "        avgValAcc.append(val_acc_list[best_epoch])\n",
    "        avgValLoss.append(val_loss_list[best_epoch])\n",
    "        avgTestAcc.append(test_acc)\n",
    "        avgTestLoss.append(test_loss)\n",
    "                                                           \n",
    "        # Write final Mask update to file if SET/RigL\n",
    "        if sparse_args != {}:\n",
    "            layer_fired_weights, total_fired_weights = mask.fired_masks_update()\n",
    "            for name in layer_fired_weights:\n",
    "                f.write('The final percentage of fired weights in the layer', name, 'is:', layer_fired_weights[name])\n",
    "            f.write('The final percentage of the total fired weights is:', total_fired_weights)\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "    # save average results for this run    \n",
    "    with open(resultsDir / Path(weightFileName + f'_avg.txt'), 'w') as f:\n",
    "        \n",
    "        f.write(f\"Repeats: {repeats}\\n\")\n",
    "        f.write(f\"Average Training Accuracy: {np.mean(avgTrainAcc)}\\n\")\n",
    "        f.write(f\"Stdev Training Accuracy: {np.std(avgTrainAcc)}\\n\")\n",
    "        f.write(f\"Average Training Loss: {np.mean(avgTrainLoss)}\\n\")\n",
    "        f.write(f\"Stdev Training Loss: {np.std(avgTrainLoss)}\\n\")\n",
    "        f.write(f\"Average Validation Accuracy: {np.mean(avgValAcc)}\\n\")\n",
    "        f.write(f\"Stdev Validation Accuracy: {np.std(avgValAcc)}\\n\")\n",
    "        f.write(f\"Average Validation Loss: {np.mean(avgValLoss)}\\n\")\n",
    "        f.write(f\"Stdev Validation Loss: {np.std(avgValLoss)}\\n\")\n",
    "        f.write(f\"Average Test Accuracy: {np.mean(avgTestAcc)}\\n\")\n",
    "        f.write(f\"Stdev Test Accuracy: {np.std(avgTestAcc)}\\n\")\n",
    "        f.write(f\"Average Test Loss: {np.mean(avgTestLoss)}\\n\")\n",
    "        f.write(f\"Stdev Test Loss: {np.std(avgTestLoss)}\\n\")\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed67ad0-8a7e-4b91-bf75-49b38a5cbd5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-05T03:09:43.905158Z",
     "iopub.status.busy": "2022-05-05T03:09:43.904772Z",
     "iopub.status.idle": "2022-05-05T03:26:07.072013Z",
     "shell.execute_reply": "2022-05-05T03:26:07.070687Z",
     "shell.execute_reply.started": "2022-05-05T03:09:43.905117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing biases...\n",
      "Removing bn1.bias of size torch.Size([64]) with 64 parameters...\n",
      "Removing layer1.0.bn1.bias of size torch.Size([64]) with 64 parameters...\n",
      "Removing layer1.0.bn2.bias of size torch.Size([64]) with 64 parameters...\n",
      "Removing layer1.1.bn1.bias of size torch.Size([64]) with 64 parameters...\n",
      "Removing layer1.1.bn2.bias of size torch.Size([64]) with 64 parameters...\n",
      "Removing layer2.0.bn1.bias of size torch.Size([128]) with 128 parameters...\n",
      "Removing layer2.0.bn2.bias of size torch.Size([128]) with 128 parameters...\n",
      "Removing layer2.0.shortcut.1.bias of size torch.Size([128]) with 128 parameters...\n",
      "Removing layer2.1.bn1.bias of size torch.Size([128]) with 128 parameters...\n",
      "Removing layer2.1.bn2.bias of size torch.Size([128]) with 128 parameters...\n",
      "Removing layer3.0.bn1.bias of size torch.Size([256]) with 256 parameters...\n",
      "Removing layer3.0.bn2.bias of size torch.Size([256]) with 256 parameters...\n",
      "Removing layer3.0.shortcut.1.bias of size torch.Size([256]) with 256 parameters...\n",
      "Removing layer3.1.bn1.bias of size torch.Size([256]) with 256 parameters...\n",
      "Removing layer3.1.bn2.bias of size torch.Size([256]) with 256 parameters...\n",
      "Removing layer4.0.bn1.bias of size torch.Size([512]) with 512 parameters...\n",
      "Removing layer4.0.bn2.bias of size torch.Size([512]) with 512 parameters...\n",
      "Removing layer4.0.shortcut.1.bias of size torch.Size([512]) with 512 parameters...\n",
      "Removing layer4.1.bn1.bias of size torch.Size([512]) with 512 parameters...\n",
      "Removing layer4.1.bn2.bias of size torch.Size([512]) with 512 parameters...\n",
      "Removed 20 layers.\n",
      "Removing 2D batch norms...\n",
      "Removing bn1 of size torch.Size([64]) = 64 parameters.\n",
      "Removing layer1.0.bn1 of size torch.Size([64]) = 64 parameters.\n",
      "Removing layer1.0.bn2 of size torch.Size([64]) = 64 parameters.\n",
      "Removing layer1.1.bn1 of size torch.Size([64]) = 64 parameters.\n",
      "Removing layer1.1.bn2 of size torch.Size([64]) = 64 parameters.\n",
      "Removing layer2.0.bn1 of size torch.Size([128]) = 128 parameters.\n",
      "Removing layer2.0.bn2 of size torch.Size([128]) = 128 parameters.\n",
      "Removing layer2.0.shortcut.1 of size torch.Size([128]) = 128 parameters.\n",
      "Removing layer2.1.bn1 of size torch.Size([128]) = 128 parameters.\n",
      "Removing layer2.1.bn2 of size torch.Size([128]) = 128 parameters.\n",
      "Removing layer3.0.bn1 of size torch.Size([256]) = 256 parameters.\n",
      "Removing layer3.0.bn2 of size torch.Size([256]) = 256 parameters.\n",
      "Removing layer3.0.shortcut.1 of size torch.Size([256]) = 256 parameters.\n",
      "Removing layer3.1.bn1 of size torch.Size([256]) = 256 parameters.\n",
      "Removing layer3.1.bn2 of size torch.Size([256]) = 256 parameters.\n",
      "Removing layer4.0.bn1 of size torch.Size([512]) = 512 parameters.\n",
      "Removing layer4.0.bn2 of size torch.Size([512]) = 512 parameters.\n",
      "Removing layer4.0.shortcut.1 of size torch.Size([512]) = 512 parameters.\n",
      "Removing layer4.1.bn1 of size torch.Size([512]) = 512 parameters.\n",
      "Removing layer4.1.bn2 of size torch.Size([512]) = 512 parameters.\n",
      "Removing 1D batch norms...\n",
      "initialize by ERK\n",
      "Sparsity of var:classifier.weight_orig had to be set to 0.\n",
      "Sparsity of var:conv1.weight_orig had to be set to 0.\n",
      "Sparsity of var:layer2.0.shortcut.0.weight_orig had to be set to 0.\n",
      "Sparsity of var:layer3.0.shortcut.0.weight_orig had to be set to 0.\n",
      "Sparsity of var:layer4.0.shortcut.0.weight_orig had to be set to 0.\n",
      "Sparsity of var:layer1.0.conv1.weight_orig had to be set to 0.\n",
      "Sparsity of var:layer1.0.conv2.weight_orig had to be set to 0.\n",
      "Sparsity of var:layer1.1.conv1.weight_orig had to be set to 0.\n",
      "Sparsity of var:layer1.1.conv2.weight_orig had to be set to 0.\n",
      "layer: conv1.weight_orig, shape: torch.Size([64, 3, 3, 3]), density: 1.0\n",
      "layer: layer1.0.conv1.weight_orig, shape: torch.Size([64, 64, 3, 3]), density: 1.0\n",
      "layer: layer1.0.conv2.weight_orig, shape: torch.Size([64, 64, 3, 3]), density: 1.0\n",
      "layer: layer1.1.conv1.weight_orig, shape: torch.Size([64, 64, 3, 3]), density: 1.0\n",
      "layer: layer1.1.conv2.weight_orig, shape: torch.Size([64, 64, 3, 3]), density: 1.0\n",
      "layer: layer2.0.conv1.weight_orig, shape: torch.Size([128, 64, 3, 3]), density: 0.7538409157832742\n",
      "layer: layer2.0.conv2.weight_orig, shape: torch.Size([128, 128, 3, 3]), density: 0.498753333169742\n",
      "layer: layer2.0.shortcut.0.weight_orig, shape: torch.Size([128, 64, 1, 1]), density: 1.0\n",
      "layer: layer2.1.conv1.weight_orig, shape: torch.Size([128, 128, 3, 3]), density: 0.498753333169742\n",
      "layer: layer2.1.conv2.weight_orig, shape: torch.Size([128, 128, 3, 3]), density: 0.498753333169742\n",
      "layer: layer3.0.conv1.weight_orig, shape: torch.Size([256, 128, 3, 3]), density: 0.37120954186297594\n",
      "layer: layer3.0.conv2.weight_orig, shape: torch.Size([256, 256, 3, 3]), density: 0.24652120857054044\n",
      "layer: layer3.0.shortcut.0.weight_orig, shape: torch.Size([256, 128, 1, 1]), density: 1.0\n",
      "layer: layer3.1.conv1.weight_orig, shape: torch.Size([256, 256, 3, 3]), density: 0.24652120857054044\n",
      "layer: layer3.1.conv2.weight_orig, shape: torch.Size([256, 256, 3, 3]), density: 0.24652120857054044\n",
      "layer: layer4.0.conv1.weight_orig, shape: torch.Size([512, 256, 3, 3]), density: 0.1841770419243227\n",
      "layer: layer4.0.conv2.weight_orig, shape: torch.Size([512, 512, 3, 3]), density: 0.12254673978168756\n",
      "layer: layer4.0.shortcut.0.weight_orig, shape: torch.Size([512, 256, 1, 1]), density: 1.0\n",
      "layer: layer4.1.conv1.weight_orig, shape: torch.Size([512, 512, 3, 3]), density: 0.12254673978168756\n",
      "layer: layer4.1.conv2.weight_orig, shape: torch.Size([512, 512, 3, 3]), density: 0.12254673978168756\n",
      "layer: classifier.weight_orig, shape: torch.Size([10, 512]), density: 1.0\n",
      "Overall sparsity 0.19999999999999996\n",
      "Total Model parameters: 11164352\n",
      "Total parameters under sparsity level of 0.19999999999999996: 0.20014811428374885\n",
      "Epoch: 1\n",
      "Train Loss: 0.000047, Acc: 0.317375\n",
      "Val Loss: 1.650078,  Acc: 0.399700\n",
      "Took 1.30 minutes.\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.000035, Acc: 0.486350\n",
      "Val Loss: 1.349824,  Acc: 0.505300\n",
      "Took 1.30 minutes.\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.000028, Acc: 0.588625\n",
      "Val Loss: 1.181718,  Acc: 0.581300\n",
      "Took 1.30 minutes.\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.000024, Acc: 0.659400\n",
      "Val Loss: 1.085786,  Acc: 0.642300\n",
      "Took 1.30 minutes.\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.000021, Acc: 0.704400\n",
      "Val Loss: 0.905937,  Acc: 0.697500\n",
      "Took 1.30 minutes.\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.000018, Acc: 0.742875\n",
      "Val Loss: 0.942097,  Acc: 0.693000\n",
      "Took 1.29 minutes.\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.000016, Acc: 0.771100\n",
      "Val Loss: 0.711353,  Acc: 0.755700\n",
      "Took 1.30 minutes.\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.000015, Acc: 0.797400\n",
      "Val Loss: 0.720369,  Acc: 0.758500\n",
      "Took 1.29 minutes.\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.000013, Acc: 0.813700\n",
      "Val Loss: 0.572264,  Acc: 0.807600\n",
      "Took 1.30 minutes.\n",
      "\n",
      "Epoch: 10\n",
      "Total Model parameters: 11164352\n",
      "Overall sparsity 0.2000367777726822\n",
      "Layerwise percentage of the fired weights of conv1.weight_orig is: 1.0\n",
      "Layerwise percentage of the fired weights of layer1.0.conv1.weight_orig is: 1.0\n",
      "Layerwise percentage of the fired weights of layer1.0.conv2.weight_orig is: 1.0\n",
      "Layerwise percentage of the fired weights of layer1.1.conv1.weight_orig is: 1.0\n",
      "Layerwise percentage of the fired weights of layer1.1.conv2.weight_orig is: 1.0\n",
      "Layerwise percentage of the fired weights of layer2.0.conv1.weight_orig is: 0.9020453559027778\n",
      "Layerwise percentage of the fired weights of layer2.0.conv2.weight_orig is: 0.66253662109375\n",
      "Layerwise percentage of the fired weights of layer2.0.shortcut.0.weight_orig is: 1.0\n",
      "Layerwise percentage of the fired weights of layer2.1.conv1.weight_orig is: 0.6655815972222222\n",
      "Layerwise percentage of the fired weights of layer2.1.conv2.weight_orig is: 0.6609157986111112\n",
      "Layerwise percentage of the fired weights of layer3.0.conv1.weight_orig is: 0.5134006076388888\n",
      "Layerwise percentage of the fired weights of layer3.0.conv2.weight_orig is: 0.3525305853949653\n",
      "Layerwise percentage of the fired weights of layer3.0.shortcut.0.weight_orig is: 1.0\n",
      "Layerwise percentage of the fired weights of layer3.1.conv1.weight_orig is: 0.3538530137803819\n",
      "Layerwise percentage of the fired weights of layer3.1.conv2.weight_orig is: 0.3525034586588542\n",
      "Layerwise percentage of the fired weights of layer4.0.conv1.weight_orig is: 0.2661624484592014\n",
      "Layerwise percentage of the fired weights of layer4.0.conv2.weight_orig is: 0.17969979180230033\n",
      "Layerwise percentage of the fired weights of layer4.0.shortcut.0.weight_orig is: 1.0\n",
      "Layerwise percentage of the fired weights of layer4.1.conv1.weight_orig is: 0.17913394504123265\n",
      "Layerwise percentage of the fired weights of layer4.1.conv2.weight_orig is: 0.17984178331163195\n",
      "Layerwise percentage of the fired weights of classifier.weight_orig is: 1.0\n",
      "The percentage of the total fired weights is: 0.2729200046720132\n",
      "conv1.weight_orig: 1728.0->1728, density: 1.000\n",
      "layer1.0.conv1.weight_orig: 36864.0->36864, density: 1.000\n",
      "layer1.0.conv2.weight_orig: 36864.0->36864, density: 1.000\n",
      "layer1.1.conv1.weight_orig: 36864.0->36864, density: 1.000\n",
      "layer1.1.conv2.weight_orig: 36864.0->36864, density: 1.000\n",
      "layer2.0.conv1.weight_orig: 55426.0->55518, density: 0.753\n",
      "layer2.0.conv2.weight_orig: 73324.0->73149, density: 0.496\n",
      "layer2.0.shortcut.0.weight_orig: 8192.0->8192, density: 1.000\n",
      "layer2.1.conv1.weight_orig: 73621.0->73818, density: 0.501\n",
      "layer2.1.conv2.weight_orig: 73171.0->73051, density: 0.495\n",
      "layer3.0.conv1.weight_orig: 109334.0->109536, density: 0.371\n",
      "layer3.0.conv2.weight_orig: 145966.0->145521, density: 0.247\n",
      "layer3.0.shortcut.0.weight_orig: 32768.0->32768, density: 1.000\n",
      "layer3.1.conv1.weight_orig: 146211.0->146237, density: 0.248\n",
      "layer3.1.conv2.weight_orig: 145706.0->145712, density: 0.247\n",
      "layer4.0.conv1.weight_orig: 217051.0->217057, density: 0.184\n",
      "layer4.0.conv2.weight_orig: 289623.0->289365, density: 0.123\n",
      "layer4.0.shortcut.0.weight_orig: 131072.0->131072, density: 1.000\n",
      "layer4.1.conv1.weight_orig: 288733.0->288607, density: 0.122\n",
      "layer4.1.conv2.weight_orig: 290022.0->289374, density: 0.123\n",
      "classifier.weight_orig: 5120.0->5120, density: 1.000\n",
      "Death rate: 0.497218027110192\n",
      "\n",
      "Train Loss: 0.000012, Acc: 0.826800\n",
      "Val Loss: 0.630110,  Acc: 0.793600\n",
      "Took 1.29 minutes.\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.000012, Acc: 0.835250\n",
      "Val Loss: 0.711979,  Acc: 0.768800\n",
      "Took 1.29 minutes.\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.000011, Acc: 0.847000\n",
      "Val Loss: 0.683214,  Acc: 0.783300\n",
      "Took 1.29 minutes.\n",
      "\n",
      "Epoch: 13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m repeats \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# function to run training and evaluation loop\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparsity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweightFileName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(dataset, noise_rate, noise_mode, sparsity, batch_size, datapath, noise_file, weightFileName, repeats, sparse_args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    130\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 132\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m, Acc: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_loss, train_acc))\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data_loader, device, loss_criterion, mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: mask\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# required for Sparse Evolutionary Training\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 39\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_x)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# calculate training accuracy\u001b[39;00m\n\u001b[1;32m     41\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "                                                           \n",
    "# Arguments to specify SET vs. RigL based on ITOP code base\n",
    "# these are settings for training SET\n",
    "sparse_args = {'multiplier': 1, \n",
    "              'decay_frequency':30000,\n",
    "               'update_frequency':1500,\n",
    "              'death_rate':0.50,\n",
    "              'death_mode': 'magnitude',\n",
    "              'growth_mode':'random',\n",
    "              'redistribution':'none',\n",
    "              'sparse_init':'ERK',\n",
    "              'fix': False}\n",
    "\n",
    "# SET, 20% Density, CIFAR10 with no noise\n",
    "dataset = \"cifar10\"\n",
    "noise_rate = 0.0\n",
    "noise_mode=\"sym\"\n",
    "batch_size=128\n",
    "datapath=datapath\n",
    "noise_file = \"NoNoiseCifar10\"\n",
    "sparsity = 0.8\n",
    "weightFileName = f\"R18_Cifar10_sparseSET_{int(noise_rate*100)}pct_{noise_mode}\"\n",
    "repeats = 3\n",
    "\n",
    "# function to run training and evaluation loop\n",
    "run(dataset, noise_rate, noise_mode, sparsity, batch_size, \n",
    "    datapath, noise_file, weightFileName, repeats, sparse_args=sparse_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a64e3f-bd86-4952-a942-c94c066ff8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
